{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main .ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AzdoSEO3vP3o","colab_type":"text"},"source":["\n","**Initial the package and googledrive**"]},{"cell_type":"code","metadata":{"id":"iZo0D309urNA","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks\n","!pwd\n","\n","#poyglot package\n","!pip install PyICU\n","!pip install pycld2\n","!pip install morfessor\n","!pip install polyglot\n","\n","#download package for word embedding\n","!polyglot download sgns2.en\n","!polyglot download sgns2.th\n","!polyglot download sgns2.ja\n","\n","!pip install tika\n","\n","#install additional fonts\n","!apt-get update\n","!apt-get install fonts-thai-tlwg  #Thai fonts\n","!apt-get install fonts-takao-mincho  #Japanese fonts\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8fZcbfFFaUZU","colab_type":"text"},"source":["#== (2) Language Detection =="]},{"cell_type":"code","metadata":{"id":"UOBwRCg4uRSe","colab_type":"code","colab":{}},"source":["print(\"\\n== (2) Language Detection ==\\n\")\n","#word2vec and langualge detection\n","import polyglot\n","from polyglot.text import Text, Word, WordList\n","\n","#reading the PDF file\n","import tika\n","from tika import parser\n","tika.initVM()\n","\n","#read the system files\n","from os import listdir\n","from os.path import isfile, join\n","\n","# ##### Traget directory #####\n","TeagetDir = './files/documents/'\n","\n","#List of all files in directory\n","allFiles = [f for f in listdir(TeagetDir) if isfile(join(TeagetDir, f))]\n","\n","langDetectResult = []\n","content = []\n","i=0\n","#loop for all files \n","for file in allFiles:\n","  print(f'\\nLanguage detection:{file} [{i+1} of {len(allFiles)}]')\n","\n","  #read PDF file content\n","  raw = parser.from_file(f'./files/documents/{file}')\n","  text = Text(raw['content'])\n","\n","  #detect Language\n","  print(f'Detect language: {text.language.name}')\n","  content.append(raw['content']) # add raw content into list\n","  langDetectResult.append(text.language.code) # save the language detection result\n","  i+=1\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0w7lbOOIae1W","colab_type":"text"},"source":["#== (3) Text processing =="]},{"cell_type":"code","metadata":{"id":"7fUmI7R6yuEG","colab_type":"code","colab":{}},"source":["print(\"\\n== (3) Text processing ==\\n\")\n","\n","import re\n","import os\n","import sys\n","from string import punctuation\n","\n","#for reading stopwords_iso\n","import json\n","\n","#Stopwords-ISO from: https://github.com/stopwords-iso/stopwords-iso\n","stopwordsISOUrl = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-iso/master/stopwords-iso.json'\n","stopwordsISOFilename = './files/stopwords-iso.json'\n","os.system(f\"wget '{stopwordsISOUrl}' -O '{stopwordsISOFilename}'\")\n","\n","with open(stopwordsISOFilename, 'r') as f:\n","  stopwordsISO = json.load(f)\n","\n","# (information from https://blog.ekbana.com/pre-processing-text-in-python-ad13ea544dae)\n","for i in range(len(content)):\n"," \n","  print(f'\\nText processing: {allFiles[i]} [{i+1} of {len(allFiles)}]')\n","  originalSize = sys.getsizeof(content[i])\n","  print(f'original size:\\t{originalSize} [byte]')\n","\n","  #save to temporary variable\n","  strContent = content[i]\n","\n","  #remove Tags\n","  strContent = re.sub('<[^<]+?>','', strContent)\n","\n","  #remove url links\n","  strContent = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', strContent, flags=re.MULTILINE)\n","\n","  #remove Number\n","  strContent = ''.join(c for c in strContent if not c.isdigit())\n","\n","  #remove punctuation\n","  strContent =  ''.join(c for c in strContent if c not in punctuation)\n","\n","\n","  #to lowercase (english)\n","  if langDetectResult[i] == 'en':\n","    strContent = strContent.lower()\n","\n","  #tokenization\n","  wordsContent = Text(strContent).words\n","\n","  #removing stop words\n","  stopword = stopwordsISO[langDetectResult[i]] # select the stopword language\n","  wordsContent = [word for word in wordsContent if word not in stopword]\n","\n","  content[i] = wordsContent\n","  processedSize = sys.getsizeof(content[i])\n","  print(f'processed size:\\t{processedSize} [byte]')\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7FKL3Ote0Yz","colab_type":"text"},"source":["#== (4) Cluster documents into logical groups  =="]},{"cell_type":"markdown","metadata":{"id":"dQiyrOy_NnSU","colab_type":"text"},"source":["Calculate document vector by using Word Vector Averaging"]},{"cell_type":"code","metadata":{"id":"s_VNFJHlW1oN","colab_type":"code","colab":{}},"source":["print(\"\\n== (4) Cluster documents into logical groups  ==\\n\")\n","from polyglot.text import Text, Word, WordList\n","import numpy as np\n","\n","\n","# ---- calculate document vector by using Word Vector Averaging ----\n","vectorResult = []\n","for fileNum in range(len(content)):\n","  print(f'Calculate average vector of: {allFiles[fileNum]} [{fileNum+1} of {len(allFiles)}]')\n","  tmpVector = []\n","  wl = WordList(content[fileNum],language =langDetectResult[fileNum])\n","  for wordNum in range(len(wl)):\n","    try:\n","      tmpVector.append(wl[wordNum].vector)\n","    except:\n","      pass\n","  avrVec = np.average(tmpVector, axis=0)\n","  vectorResult.append(avrVec)\n","vectorResult = np.array(vectorResult)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jYr1lsFzNwDr","colab_type":"text"},"source":["Cluster the document using Affinity propagation"]},{"cell_type":"code","metadata":{"id":"aCABMiOXNwdt","colab_type":"code","colab":{}},"source":["# ---- Cluster the document using Affinity propagation ----\n","from sklearn.cluster import AffinityPropagation\n","from sklearn import metrics\n","from sklearn import preprocessing\n","\n","#X = preprocessing.normalize(vectorResult,axis=0)\n","X = vectorResult\n","\n","# Compute Affinity Propagation\n","af = AffinityPropagation(preference=-50).fit(X)\n","cluster_centers_indices = af.cluster_centers_indices_\n","labels = af.labels_\n","\n","n_clusters_ = len(cluster_centers_indices)\n","\n","print('Estimated number of clusters: %d' % n_clusters_)\n","\n","# #############################################################################\n","# print all the result and save result file\n","outText = \"filename,language,label\\n\"\n","for i in range(len(allFiles)):\n","  print(f'{allFiles[i]} \\t label: {labels[i]}')\n","  outText += f'{allFiles[i]},{langDetectResult[i]},{labels[i]}\\n' \n","\n","#save to result.csv\n","with open(\"./files/result.csv\", \"w\") as f:\n","  f.write(outText)\n","\n","# #############################################################################\n","# Plot result\n","import matplotlib.pyplot as plt\n","from itertools import cycle\n","\n","plt.close('all')\n","plt.figure(1)\n","plt.clf()\n","\n","colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n","for k, col in zip(range(n_clusters_), colors):\n","    class_members = labels == k\n","    cluster_center = X[cluster_centers_indices[k]]\n","    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n","    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n","             markeredgecolor='k', markersize=14)\n","    for x in X[class_members]:\n","        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n","\n","plt.title('Estimated number of clusters: %d' % n_clusters_)\n","plt.show()\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKq89On0Pxy1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}